{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark \n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+\n",
      "|days|weeks|years|names|\n",
      "+----+-----+-----+-----+\n",
      "|   1|    5|    0|   aa|\n",
      "|   2|    6|    0|   bb|\n",
      "|   3|    4|    0|   ab|\n",
      "|   4|    3|    0|   ba|\n",
      "|   5|    8|    1|   cc|\n",
      "|   6|    9|    1|   dd|\n",
      "+----+-----+-----+-----+\n",
      "\n",
      "+----+-----+-----+-----+\n",
      "|days|weeks|years|names|\n",
      "+----+-----+-----+-----+\n",
      "|   1|    5|    0|   aa|\n",
      "|   2|    6|    0|   bb|\n",
      "|   3|    4|    0|   ab|\n",
      "|   4|    3|    0|   ba|\n",
      "+----+-----+-----+-----+\n",
      "\n",
      "+----+-----+---------------------+-----+\n",
      "|days|weeks|years served sentence|names|\n",
      "+----+-----+---------------------+-----+\n",
      "|   1|    5|                   10|   aa|\n",
      "|   2|    6|                   10|   bb|\n",
      "|   3|    4|                   10|   ab|\n",
      "|   4|    3|                   10|   ba|\n",
      "|   5|    8|                   11|   cc|\n",
      "|   6|    9|                   11|   dd|\n",
      "+----+-----+---------------------+-----+\n",
      "\n",
      "+---------------------+-----+\n",
      "|years served sentence|count|\n",
      "+---------------------+-----+\n",
      "|                    1|    2|\n",
      "|                    0|    4|\n",
      "+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Implement a PySpark script that applies transformations like filter and withColumn on a DataFrame\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sp = SparkSession.builder.appName('dataframe_transforms').getOrCreate()\n",
    "#header = False by default, does not read column names, if schema is not referred then it taks every col dtype as string by default\n",
    "df = sp.read.csv('trial.csv', header = True, inferSchema = True)\n",
    "df.show()\n",
    "\n",
    "#filter operation \n",
    "df.filter(df.years == 0).show()\n",
    "\n",
    "#withColumn operation\n",
    "df = df.withColumnRenamed('years', 'years served sentence')\n",
    "df.withColumn(\"years served sentence\", col(\"years served sentence\") + 10).show()\n",
    "\n",
    "#groupBy\n",
    "df.groupBy('years served sentence').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataframe is 6\n",
      "The number of entries with the name starting with \"a\" is: 2 entries\n",
      "+----+-----+---------------------+-----+\n",
      "|days|weeks|years served sentence|names|\n",
      "+----+-----+---------------------+-----+\n",
      "|   1|    5|                    0|   aa|\n",
      "|   3|    4|                    0|   ab|\n",
      "+----+-----+---------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Write a PySpark script that performs actions like count and show on a DataFrame.\n",
    "print(\"The length of the dataframe is\", df.count())\n",
    "df_mod = df.filter(df.names.startswith('a'))\n",
    "print('The number of entries with the name starting with \"a\" is: ' + str(df_mod.count()) + ' entries')\n",
    "df_mod.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of weeks a prisoner has served is:\n",
      "+----------+\n",
      "|max(weeks)|\n",
      "+----------+\n",
      "|         9|\n",
      "+----------+\n",
      "\n",
      "The average number of years served by prisoners is:\n",
      "+--------------------------+\n",
      "|avg(years served sentence)|\n",
      "+--------------------------+\n",
      "|        0.3333333333333333|\n",
      "+--------------------------+\n",
      "\n",
      "The sum of the total years served in prison is: \n",
      "+--------------------------+\n",
      "|sum(years served sentence)|\n",
      "+--------------------------+\n",
      "|                         2|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3) Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame.\n",
    "from pyspark.sql.functions import sum as sum1\n",
    "from pyspark.sql.functions import avg \n",
    "from pyspark.sql.functions import max as max1\n",
    "\n",
    "print(\"The maximum number of weeks a prisoner has served is:\")\n",
    "df.select(max1(\"weeks\")).show()\n",
    "print(\"The average number of years served by prisoners is:\")\n",
    "df.select(avg(\"years served sentence\")).show()\n",
    "print(\"The sum of the total years served in prison is: \")\n",
    "df.select(sum1(\"years served sentence\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Show how to write a PySpark DataFrame to a CSV file.\n",
    "df.write.options(header = 'True').mode('overwrite').csv('updated_data.csv')\n",
    "sp.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Transformations i...|\n",
      "|DataFrame from an...|\n",
      "|meaning they are ...|\n",
      "|Some common trans...|\n",
      "+--------------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|           word|count|\n",
      "+---------------+-----+\n",
      "|            are|    3|\n",
      "|             in|    2|\n",
      "|             is|    2|\n",
      "|        PySpark|    2|\n",
      "|             an|    2|\n",
      "|        include|    1|\n",
      "|     evaluation|    1|\n",
      "|           lazy|    1|\n",
      "|            not|    1|\n",
      "|         blocks|    1|\n",
      "|            new|    1|\n",
      "|          their|    1|\n",
      "|         action|    1|\n",
      "|           they|    1|\n",
      "|       building|    1|\n",
      "|      Resilient|    1|\n",
      "|Transformations|    1|\n",
      "|           one.|    1|\n",
      "|            but|    1|\n",
      "|      paradigm,|    1|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "The total word count of the text file is\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|        51|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 Implement wordcount program in PySpark.\n",
    "from pyspark.sql.functions import explode \n",
    "from pyspark.sql.functions import split \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#create dataframe with word count per word\n",
    "ss = SparkSession.builder.appName('wordcount_program').getOrCreate()\n",
    "df = ss.read.text('lab2wc.txt')\n",
    "df.show()\n",
    "df_count = (df.withColumn('word', explode(split(col('value'), ' '))).groupBy('word').count().sort('count', ascending = False))\n",
    "df_count.show()\n",
    "\n",
    "#display total count\n",
    "print('The total word count of the text file is')\n",
    "df_count.select(sum1('count')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
