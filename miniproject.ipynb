{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark \n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lplab/anaconda3/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------+--------+---+---+---+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----+----+\n",
      "|                 _c0|   _c1|                _c2|     _c3|_c4|_c5|_c6|           _c7| _c8|              _c9|         _c10|         _c11|               _c12|           _c13|_c14|_c15|\n",
      "+--------------------+------+-------------------+--------+---+---+---+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----+----+\n",
      "|{F887F88E-7D15-44...| 70000|1995-07-07 00:00:00|MK15 9HP|  D|  N|  F|            31|null|    ALDRICH DRIVE|       WILLEN|MILTON KEYNES|      MILTON KEYNES|  MILTON KEYNES|   A|   A|\n",
      "|{40FD4DF2-5362-40...| 44500|1995-02-03 00:00:00| SR6 0AQ|  T|  N|  F|            50|null|      HOWICK PARK|   SUNDERLAND|   SUNDERLAND|         SUNDERLAND|  TYNE AND WEAR|   A|   A|\n",
      "|{7A99F89E-7D81-4E...| 56500|1995-01-13 00:00:00| CO6 1SQ|  T|  N|  F|            19|null| BRICK KILN CLOSE|   COGGESHALL|   COLCHESTER|          BRAINTREE|          ESSEX|   A|   A|\n",
      "|{28225260-E61C-4E...| 58000|1995-07-28 00:00:00| B90 4TG|  T|  N|  F|            37|null| RAINSBROOK DRIVE|      SHIRLEY|     SOLIHULL|           SOLIHULL|  WEST MIDLANDS|   A|   A|\n",
      "|{444D34D7-9BA6-43...| 51000|1995-06-28 00:00:00| DY5 1SA|  S|  N|  F|            59|null|       MERRY HILL|BRIERLEY HILL|BRIERLEY HILL|             DUDLEY|  WEST MIDLANDS|   A|   A|\n",
      "|{AE76CAF1-F8CC-43...| 17000|1995-03-10 00:00:00| S65 1QJ|  T|  N|  L|            22|null|    DENMAN STREET|    ROTHERHAM|    ROTHERHAM|          ROTHERHAM|SOUTH YORKSHIRE|   A|   A|\n",
      "|{709FB471-3690-49...| 58000|1995-04-28 00:00:00| PE7 3AL|  D|  Y|  F|             4|null|       BROOK LANE|       FARCET| PETERBOROUGH|       PETERBOROUGH| CAMBRIDGESHIRE|   A|   A|\n",
      "|{5FA8692E-537B-42...| 19500|1995-01-27 00:00:00|SK10 2QW|  T|  N|  L|            38|null|    GARDEN STREET| MACCLESFIELD| MACCLESFIELD|       MACCLESFIELD|       CHESHIRE|   A|   A|\n",
      "|{E78710AD-ED1A-4B...| 20000|1995-01-16 00:00:00| SA6 5AY|  D|  N|  F|           592|null|     CLYDACH ROAD|     YNYSTAWE|      SWANSEA|            SWANSEA|        SWANSEA|   A|   A|\n",
      "|{1DFBF83E-53A7-48...|137500|1995-03-31 00:00:00| NR2 2NQ|  D|  N|  F|            26|null|   LIME TREE ROAD|      NORWICH|      NORWICH|            NORWICH|        NORFOLK|   A|   A|\n",
      "|{BCD607D8-7698-4C...| 57500|1995-12-20 00:00:00| WS6 7BQ|  D|  N|  F|             6|null|    MERRILL CLOSE|      WALSALL|      WALSALL|SOUTH STAFFORDSHIRE|  STAFFORDSHIRE|   A|   A|\n",
      "|{461BAE3D-DD53-40...| 70000|1995-09-20 00:00:00|GL52 3LH|  D|  N|  F|          139B|null|    NEW BARN LANE|   CHELTENHAM|   CHELTENHAM|         CHELTENHAM|GLOUCESTERSHIRE|   A|   A|\n",
      "|{003DB740-4F22-46...| 62750|1995-10-02 00:00:00| BR3 4AT|  T|  N|  F|            56|null|        EDEN ROAD|    BECKENHAM|    BECKENHAM|            BROMLEY| GREATER LONDON|   A|   A|\n",
      "|{7BC20813-4C02-4F...| 35000|1995-07-25 00:00:00|  L9 6DZ|  T|  N|  L|            25|null|    TILSTON CLOSE|    LIVERPOOL|    LIVERPOOL|          LIVERPOOL|     MERSEYSIDE|   A|   A|\n",
      "|{8B5FB0C0-01CF-45...| 78000|1995-09-29 00:00:00|SW16 3BL|  S|  N|  F|           208|null|       GREEN LANE|       LONDON|       LONDON|            CROYDON| GREATER LONDON|   A|   A|\n",
      "|{38008828-F33E-4F...| 51000|1995-10-31 00:00:00| UB5 5LH|  S|  N|  F|            57|null|    BENGARTH ROAD|     NORTHOLT|     NORTHOLT|             EALING| GREATER LONDON|   A|   A|\n",
      "|{E104A9E7-1D6A-4D...| 36500|1995-06-09 00:00:00| FY4 1DL|  F|  N|  L|CLARENCE COURT|  28| RAWCLIFFE STREET|    BLACKPOOL|    BLACKPOOL|          BLACKPOOL|      BLACKPOOL|   A|   A|\n",
      "|{8982E939-2927-45...| 41000|1995-02-10 00:00:00|NE45 5AP|  T|  N|  F|            21|null|     FRONT STREET|    CORBRIDGE|    CORBRIDGE|           TYNEDALE| NORTHUMBERLAND|   A|   A|\n",
      "|{C6AC299D-A75A-48...|100000|1995-09-29 00:00:00| WR5 3EU|  D|  N|  F|             8|null|BARNESHALL AVENUE|    WORCESTER|    WORCESTER|          WORCESTER| WORCESTERSHIRE|   A|   A|\n",
      "|{EAFDBDAA-0C4D-4A...|123000|1995-09-05 00:00:00|NG34 7TF|  D|  Y|  F|             5|null|   CLAY HILL ROAD|     SLEAFORD|     SLEAFORD|     NORTH KESTEVEN|   LINCOLNSHIRE|   A|   A|\n",
      "+--------------------+------+-------------------+--------+---+---+---+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load housing dataset from csv file\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "ss = SparkSession.builder.appName('housing_data').getOrCreate()\n",
    "df_without_header = ss.read.option('inferSchema', True).option('header', False).csv('housing_data.csv')\n",
    "df_without_header.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------+-------------------+--------+-------------+-------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+---------------------------------+\n",
      "|Transaction_unique_identifier| price|   Date_of_Transfer|postcode|Property_Type|Old/New|Duration|          PAON|SAON|           Street|     Locality|    Town/City|           District|         County|PPDCategory_Type|Record_Status - monthly_file_only|\n",
      "+-----------------------------+------+-------------------+--------+-------------+-------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+---------------------------------+\n",
      "|         {F887F88E-7D15-44...| 70000|1995-07-07 00:00:00|MK15 9HP|            D|      N|       F|            31|null|    ALDRICH DRIVE|       WILLEN|MILTON KEYNES|      MILTON KEYNES|  MILTON KEYNES|               A|                                A|\n",
      "|         {40FD4DF2-5362-40...| 44500|1995-02-03 00:00:00| SR6 0AQ|            T|      N|       F|            50|null|      HOWICK PARK|   SUNDERLAND|   SUNDERLAND|         SUNDERLAND|  TYNE AND WEAR|               A|                                A|\n",
      "|         {7A99F89E-7D81-4E...| 56500|1995-01-13 00:00:00| CO6 1SQ|            T|      N|       F|            19|null| BRICK KILN CLOSE|   COGGESHALL|   COLCHESTER|          BRAINTREE|          ESSEX|               A|                                A|\n",
      "|         {28225260-E61C-4E...| 58000|1995-07-28 00:00:00| B90 4TG|            T|      N|       F|            37|null| RAINSBROOK DRIVE|      SHIRLEY|     SOLIHULL|           SOLIHULL|  WEST MIDLANDS|               A|                                A|\n",
      "|         {444D34D7-9BA6-43...| 51000|1995-06-28 00:00:00| DY5 1SA|            S|      N|       F|            59|null|       MERRY HILL|BRIERLEY HILL|BRIERLEY HILL|             DUDLEY|  WEST MIDLANDS|               A|                                A|\n",
      "|         {AE76CAF1-F8CC-43...| 17000|1995-03-10 00:00:00| S65 1QJ|            T|      N|       L|            22|null|    DENMAN STREET|    ROTHERHAM|    ROTHERHAM|          ROTHERHAM|SOUTH YORKSHIRE|               A|                                A|\n",
      "|         {709FB471-3690-49...| 58000|1995-04-28 00:00:00| PE7 3AL|            D|      Y|       F|             4|null|       BROOK LANE|       FARCET| PETERBOROUGH|       PETERBOROUGH| CAMBRIDGESHIRE|               A|                                A|\n",
      "|         {5FA8692E-537B-42...| 19500|1995-01-27 00:00:00|SK10 2QW|            T|      N|       L|            38|null|    GARDEN STREET| MACCLESFIELD| MACCLESFIELD|       MACCLESFIELD|       CHESHIRE|               A|                                A|\n",
      "|         {E78710AD-ED1A-4B...| 20000|1995-01-16 00:00:00| SA6 5AY|            D|      N|       F|           592|null|     CLYDACH ROAD|     YNYSTAWE|      SWANSEA|            SWANSEA|        SWANSEA|               A|                                A|\n",
      "|         {1DFBF83E-53A7-48...|137500|1995-03-31 00:00:00| NR2 2NQ|            D|      N|       F|            26|null|   LIME TREE ROAD|      NORWICH|      NORWICH|            NORWICH|        NORFOLK|               A|                                A|\n",
      "|         {BCD607D8-7698-4C...| 57500|1995-12-20 00:00:00| WS6 7BQ|            D|      N|       F|             6|null|    MERRILL CLOSE|      WALSALL|      WALSALL|SOUTH STAFFORDSHIRE|  STAFFORDSHIRE|               A|                                A|\n",
      "|         {461BAE3D-DD53-40...| 70000|1995-09-20 00:00:00|GL52 3LH|            D|      N|       F|          139B|null|    NEW BARN LANE|   CHELTENHAM|   CHELTENHAM|         CHELTENHAM|GLOUCESTERSHIRE|               A|                                A|\n",
      "|         {003DB740-4F22-46...| 62750|1995-10-02 00:00:00| BR3 4AT|            T|      N|       F|            56|null|        EDEN ROAD|    BECKENHAM|    BECKENHAM|            BROMLEY| GREATER LONDON|               A|                                A|\n",
      "|         {7BC20813-4C02-4F...| 35000|1995-07-25 00:00:00|  L9 6DZ|            T|      N|       L|            25|null|    TILSTON CLOSE|    LIVERPOOL|    LIVERPOOL|          LIVERPOOL|     MERSEYSIDE|               A|                                A|\n",
      "|         {8B5FB0C0-01CF-45...| 78000|1995-09-29 00:00:00|SW16 3BL|            S|      N|       F|           208|null|       GREEN LANE|       LONDON|       LONDON|            CROYDON| GREATER LONDON|               A|                                A|\n",
      "|         {38008828-F33E-4F...| 51000|1995-10-31 00:00:00| UB5 5LH|            S|      N|       F|            57|null|    BENGARTH ROAD|     NORTHOLT|     NORTHOLT|             EALING| GREATER LONDON|               A|                                A|\n",
      "|         {E104A9E7-1D6A-4D...| 36500|1995-06-09 00:00:00| FY4 1DL|            F|      N|       L|CLARENCE COURT|  28| RAWCLIFFE STREET|    BLACKPOOL|    BLACKPOOL|          BLACKPOOL|      BLACKPOOL|               A|                                A|\n",
      "|         {8982E939-2927-45...| 41000|1995-02-10 00:00:00|NE45 5AP|            T|      N|       F|            21|null|     FRONT STREET|    CORBRIDGE|    CORBRIDGE|           TYNEDALE| NORTHUMBERLAND|               A|                                A|\n",
      "|         {C6AC299D-A75A-48...|100000|1995-09-29 00:00:00| WR5 3EU|            D|      N|       F|             8|null|BARNESHALL AVENUE|    WORCESTER|    WORCESTER|          WORCESTER| WORCESTERSHIRE|               A|                                A|\n",
      "|         {EAFDBDAA-0C4D-4A...|123000|1995-09-05 00:00:00|NG34 7TF|            D|      Y|       F|             5|null|   CLAY HILL ROAD|     SLEAFORD|     SLEAFORD|     NORTH KESTEVEN|   LINCOLNSHIRE|               A|                                A|\n",
      "+-----------------------------+------+-------------------+--------+-------------+-------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add column names from kaggle dataset page\n",
    "col_names=['Transaction_unique_identifier', 'price', 'Date_of_Transfer', 'postcode', 'Property_Type', 'Old/New',\n",
    "'Duration', 'PAON', 'SAON', 'Street', 'Locality', 'Town/City', 'District', 'County', 'PPDCategory_Type',\n",
    "'Record_Status - monthly_file_only']\n",
    "housing_df = df_without_header.toDF(*col_names)\n",
    "                                    \n",
    "#use first 500000 entries due to limitations in compute power \n",
    "housing_df = ss.createDataFrame(housing_df.head(500000), housing_df.schema)\n",
    "housing_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_unique_identifier: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- Date_of_Transfer: timestamp (nullable = true)\n",
      " |-- postcode: string (nullable = true)\n",
      " |-- Property_Type: string (nullable = true)\n",
      " |-- Old/New: string (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- PAON: string (nullable = true)\n",
      " |-- SAON: string (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Locality: string (nullable = true)\n",
      " |-- Town/City: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- PPDCategory_Type: string (nullable = true)\n",
      " |-- Record_Status - monthly_file_only: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data schema \n",
    "housing_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing data contains 500000 entries\n",
      "There are 365 entries based on unique Date_of_Transfer\n"
     ]
    }
   ],
   "source": [
    "#get size stats and category statistics to decide which attributes are useful in computation\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#total entries\n",
    "print(\"Housing data contains \" + str(housing_df.count()) + \" entries\")\n",
    "\n",
    "#Date_of_Transfer\n",
    "transfer_count =  housing_df.select('Date_of_Transfer').distinct().count()\n",
    "print(\"There are \" + str(transfer_count) + \" entries based on unique Date_of_Transfer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 500000 entries based on unique Transaction_unique_identifier values\n",
      "There are 10819 entries based on unique price values\n",
      "There are 365 entries based on unique Date_of_Transfer values\n",
      "There are 316022 entries based on unique postcode values\n",
      "There are 5 entries based on unique Property_Type values\n",
      "There are 2 entries based on unique Old/New values\n",
      "There are 3 entries based on unique Duration values\n",
      "There are 32683 entries based on unique PAON values\n",
      "There are 1811 entries based on unique SAON values\n",
      "There are 118030 entries based on unique Street values\n",
      "There are 14206 entries based on unique Locality values\n",
      "There are 1156 entries based on unique Town/City values\n",
      "There are 459 entries based on unique District values\n",
      "There are 130 entries based on unique County values\n",
      "There are 2 entries based on unique PPDCategory_Type values\n",
      "There are 1 entries based on unique Record_Status - monthly_file_only values\n"
     ]
    }
   ],
   "source": [
    "def count_cols(df_col):\n",
    "    col_count = housing_df.select(df_col).distinct().count()\n",
    "    print(\"There are \" + str(col_count) + \" entries based on unique \" + str(df_col) + \" values\")\n",
    "    \n",
    "for i in col_names:\n",
    "    count_cols(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Transaction_unique_identifier due to there not being enough unique column entries\n",
    "housing_df = housing_df.drop('Transaction_unique_identifier')\n",
    "housing_df = housing_df.drop('Record_Status - monthly_file_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['postcode', 'Property_Type', 'Old/New', 'Duration', 'PAON', 'SAON', 'Street', 'Locality', 'Town/City', 'District', 'County', 'PPDCategory_Type']\n"
     ]
    }
   ],
   "source": [
    "#get list of string entries from schema \n",
    "string_type_columns = []\n",
    "for col in housing_df.dtypes:\n",
    "    #print(col[0]+\" , \"+col[1])\n",
    "    if(col[1] == 'string'):\n",
    "        string_type_columns.append(col[0])\n",
    "print(string_type_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+\n",
      "| price|   Date_of_Transfer|Duration|          PAON|SAON|           Street|     Locality|    Town/City|           District|         County|PPDCategory_Type|postcode|Property_Type|Old/New|\n",
      "+------+-------------------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+\n",
      "| 70000|1995-07-07 00:00:00|       F|            31|null|    ALDRICH DRIVE|       WILLEN|MILTON KEYNES|      MILTON KEYNES|  MILTON KEYNES|               A|211233.0|          2.0|    0.0|\n",
      "| 44500|1995-02-03 00:00:00|       F|            50|null|      HOWICK PARK|   SUNDERLAND|   SUNDERLAND|         SUNDERLAND|  TYNE AND WEAR|               A| 89227.0|          0.0|    0.0|\n",
      "| 56500|1995-01-13 00:00:00|       F|            19|null| BRICK KILN CLOSE|   COGGESHALL|   COLCHESTER|          BRAINTREE|          ESSEX|               A| 19489.0|          0.0|    0.0|\n",
      "| 58000|1995-07-28 00:00:00|       F|            37|null| RAINSBROOK DRIVE|      SHIRLEY|     SOLIHULL|           SOLIHULL|  WEST MIDLANDS|               A|107025.0|          0.0|    0.0|\n",
      "| 51000|1995-06-28 00:00:00|       F|            59|null|       MERRY HILL|BRIERLEY HILL|BRIERLEY HILL|             DUDLEY|  WEST MIDLANDS|               A|160201.0|          1.0|    0.0|\n",
      "| 17000|1995-03-10 00:00:00|       L|            22|null|    DENMAN STREET|    ROTHERHAM|    ROTHERHAM|          ROTHERHAM|SOUTH YORKSHIRE|               A| 83680.0|          0.0|    0.0|\n",
      "| 58000|1995-04-28 00:00:00|       F|             4|null|       BROOK LANE|       FARCET| PETERBOROUGH|       PETERBOROUGH| CAMBRIDGESHIRE|               A|  4487.0|          2.0|    1.0|\n",
      "| 19500|1995-01-27 00:00:00|       L|            38|null|    GARDEN STREET| MACCLESFIELD| MACCLESFIELD|       MACCLESFIELD|       CHESHIRE|               A| 85897.0|          0.0|    0.0|\n",
      "| 20000|1995-01-16 00:00:00|       F|           592|null|     CLYDACH ROAD|     YNYSTAWE|      SWANSEA|            SWANSEA|        SWANSEA|               A| 84589.0|          2.0|    0.0|\n",
      "|137500|1995-03-31 00:00:00|       F|            26|null|   LIME TREE ROAD|      NORWICH|      NORWICH|            NORWICH|        NORFOLK|               A|229145.0|          2.0|    0.0|\n",
      "| 57500|1995-12-20 00:00:00|       F|             6|null|    MERRILL CLOSE|      WALSALL|      WALSALL|SOUTH STAFFORDSHIRE|  STAFFORDSHIRE|               A|312087.0|          2.0|    0.0|\n",
      "| 70000|1995-09-20 00:00:00|       F|          139B|null|    NEW BARN LANE|   CHELTENHAM|   CHELTENHAM|         CHELTENHAM|GLOUCESTERSHIRE|               A|169901.0|          2.0|    0.0|\n",
      "| 62750|1995-10-02 00:00:00|       F|            56|null|        EDEN ROAD|    BECKENHAM|    BECKENHAM|            BROMLEY| GREATER LONDON|               A| 43245.0|          0.0|    0.0|\n",
      "| 35000|1995-07-25 00:00:00|       L|            25|null|    TILSTON CLOSE|    LIVERPOOL|    LIVERPOOL|          LIVERPOOL|     MERSEYSIDE|               A| 24197.0|          0.0|    0.0|\n",
      "| 78000|1995-09-29 00:00:00|       F|           208|null|       GREEN LANE|       LONDON|       LONDON|            CROYDON| GREATER LONDON|               A|283853.0|          1.0|    0.0|\n",
      "| 51000|1995-10-31 00:00:00|       F|            57|null|    BENGARTH ROAD|     NORTHOLT|     NORTHOLT|             EALING| GREATER LONDON|               A|301028.0|          1.0|    0.0|\n",
      "| 36500|1995-06-09 00:00:00|       L|CLARENCE COURT|  28| RAWCLIFFE STREET|    BLACKPOOL|    BLACKPOOL|          BLACKPOOL|      BLACKPOOL|               A| 56207.0|          3.0|    0.0|\n",
      "| 41000|1995-02-10 00:00:00|       F|            21|null|     FRONT STREET|    CORBRIDGE|    CORBRIDGE|           TYNEDALE| NORTHUMBERLAND|               A|218538.0|          0.0|    0.0|\n",
      "|100000|1995-09-29 00:00:00|       F|             8|null|BARNESHALL AVENUE|    WORCESTER|    WORCESTER|          WORCESTER| WORCESTERSHIRE|               A|310685.0|          2.0|    0.0|\n",
      "|123000|1995-09-05 00:00:00|       F|             5|null|   CLAY HILL ROAD|     SLEAFORD|     SLEAFORD|     NORTH KESTEVEN|   LINCOLNSHIRE|               A| 72614.0|          2.0|    1.0|\n",
      "+------+-------------------+--------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def encode_values(column):\n",
    "    global housing_df\n",
    "    indexer = StringIndexer(inputCol= column, outputCol= column + \"encoded\")\n",
    "    housing_df = indexer.fit(housing_df).transform(housing_df)\n",
    "    housing_df = housing_df.drop(column).withColumnRenamed(column + \"encoded\", column)\n",
    "    \n",
    "\"\"\"for col_name in string_type_columns:\n",
    "    encode_values(col_name)\n",
    "housing_df.show()\"\"\"    \n",
    "encode_values('postcode')\n",
    "encode_values('Property_Type')\n",
    "encode_values('Old/New')\n",
    "housing_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+--------+\n",
      "| price|   Date_of_Transfer|          PAON|SAON|           Street|     Locality|    Town/City|           District|         County|PPDCategory_Type|postcode|Property_Type|Old/New|Duration|\n",
      "+------+-------------------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+--------+\n",
      "| 70000|1995-07-07 00:00:00|            31|null|    ALDRICH DRIVE|       WILLEN|MILTON KEYNES|      MILTON KEYNES|  MILTON KEYNES|               A|211233.0|          2.0|    0.0|     0.0|\n",
      "| 44500|1995-02-03 00:00:00|            50|null|      HOWICK PARK|   SUNDERLAND|   SUNDERLAND|         SUNDERLAND|  TYNE AND WEAR|               A| 89227.0|          0.0|    0.0|     0.0|\n",
      "| 56500|1995-01-13 00:00:00|            19|null| BRICK KILN CLOSE|   COGGESHALL|   COLCHESTER|          BRAINTREE|          ESSEX|               A| 19489.0|          0.0|    0.0|     0.0|\n",
      "| 58000|1995-07-28 00:00:00|            37|null| RAINSBROOK DRIVE|      SHIRLEY|     SOLIHULL|           SOLIHULL|  WEST MIDLANDS|               A|107025.0|          0.0|    0.0|     0.0|\n",
      "| 51000|1995-06-28 00:00:00|            59|null|       MERRY HILL|BRIERLEY HILL|BRIERLEY HILL|             DUDLEY|  WEST MIDLANDS|               A|160201.0|          1.0|    0.0|     0.0|\n",
      "| 17000|1995-03-10 00:00:00|            22|null|    DENMAN STREET|    ROTHERHAM|    ROTHERHAM|          ROTHERHAM|SOUTH YORKSHIRE|               A| 83680.0|          0.0|    0.0|     1.0|\n",
      "| 58000|1995-04-28 00:00:00|             4|null|       BROOK LANE|       FARCET| PETERBOROUGH|       PETERBOROUGH| CAMBRIDGESHIRE|               A|  4487.0|          2.0|    1.0|     0.0|\n",
      "| 19500|1995-01-27 00:00:00|            38|null|    GARDEN STREET| MACCLESFIELD| MACCLESFIELD|       MACCLESFIELD|       CHESHIRE|               A| 85897.0|          0.0|    0.0|     1.0|\n",
      "| 20000|1995-01-16 00:00:00|           592|null|     CLYDACH ROAD|     YNYSTAWE|      SWANSEA|            SWANSEA|        SWANSEA|               A| 84589.0|          2.0|    0.0|     0.0|\n",
      "|137500|1995-03-31 00:00:00|            26|null|   LIME TREE ROAD|      NORWICH|      NORWICH|            NORWICH|        NORFOLK|               A|229145.0|          2.0|    0.0|     0.0|\n",
      "| 57500|1995-12-20 00:00:00|             6|null|    MERRILL CLOSE|      WALSALL|      WALSALL|SOUTH STAFFORDSHIRE|  STAFFORDSHIRE|               A|312087.0|          2.0|    0.0|     0.0|\n",
      "| 70000|1995-09-20 00:00:00|          139B|null|    NEW BARN LANE|   CHELTENHAM|   CHELTENHAM|         CHELTENHAM|GLOUCESTERSHIRE|               A|169901.0|          2.0|    0.0|     0.0|\n",
      "| 62750|1995-10-02 00:00:00|            56|null|        EDEN ROAD|    BECKENHAM|    BECKENHAM|            BROMLEY| GREATER LONDON|               A| 43245.0|          0.0|    0.0|     0.0|\n",
      "| 35000|1995-07-25 00:00:00|            25|null|    TILSTON CLOSE|    LIVERPOOL|    LIVERPOOL|          LIVERPOOL|     MERSEYSIDE|               A| 24197.0|          0.0|    0.0|     1.0|\n",
      "| 78000|1995-09-29 00:00:00|           208|null|       GREEN LANE|       LONDON|       LONDON|            CROYDON| GREATER LONDON|               A|283853.0|          1.0|    0.0|     0.0|\n",
      "| 51000|1995-10-31 00:00:00|            57|null|    BENGARTH ROAD|     NORTHOLT|     NORTHOLT|             EALING| GREATER LONDON|               A|301028.0|          1.0|    0.0|     0.0|\n",
      "| 36500|1995-06-09 00:00:00|CLARENCE COURT|  28| RAWCLIFFE STREET|    BLACKPOOL|    BLACKPOOL|          BLACKPOOL|      BLACKPOOL|               A| 56207.0|          3.0|    0.0|     1.0|\n",
      "| 41000|1995-02-10 00:00:00|            21|null|     FRONT STREET|    CORBRIDGE|    CORBRIDGE|           TYNEDALE| NORTHUMBERLAND|               A|218538.0|          0.0|    0.0|     0.0|\n",
      "|100000|1995-09-29 00:00:00|             8|null|BARNESHALL AVENUE|    WORCESTER|    WORCESTER|          WORCESTER| WORCESTERSHIRE|               A|310685.0|          2.0|    0.0|     0.0|\n",
      "|123000|1995-09-05 00:00:00|             5|null|   CLAY HILL ROAD|     SLEAFORD|     SLEAFORD|     NORTH KESTEVEN|   LINCOLNSHIRE|               A| 72614.0|          2.0|    1.0|     0.0|\n",
      "+------+-------------------+--------------+----+-----------------+-------------+-------------+-------------------+---------------+----------------+--------+-------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encode_values('Duration')\n",
    "housing_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o407.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 130.0 failed 1 times, most recent failure: Lost task 0.0 in stage 130.0 (TID 688) (lpcp-23 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$4946/0x00007fa76918c000: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4216)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$4946/0x00007fa76918c000: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1259355b9fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencode_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PAON'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mencode_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SAON'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhousing_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1323\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o407.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 130.0 failed 1 times, most recent failure: Lost task 0.0 in stage 130.0 (TID 688) (lpcp-23 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$4946/0x00007fa76918c000: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4216)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (StringIndexerModel$$Lambda$4946/0x00007fa76918c000: (string) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:396)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "encode_values('PAON')\n",
    "encode_values('SAON')\n",
    "housing_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
